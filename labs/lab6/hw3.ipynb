{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS-GA 3001.001 Special Topics in Data Science: Probabilistic Time Series Analysis\n",
    "\n",
    "# Week 6 Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import pylab\n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"./sp500w.csv\")\n",
    "data = data_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot & visualize the data\n",
    "print(data_df.head())\n",
    "print(\"Shape of the data\", data.shape)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(data[:, 0], data[:, 1], '*b', label=\"S&P 500\")\n",
    "plt.xlabel(\"Week Index\", fontsize=12)\n",
    "plt.ylabel(\"Weekly Return\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_agree(x, z):\n",
    "    \"\"\"\n",
    "    Function that shows the % of agreement among two list\n",
    "    \"\"\"\n",
    "    assert len(x)==len(z)\n",
    "    return float(np.sum(np.array(x)==np.array(z)))/len(x)\n",
    "\n",
    "class MyHMM:\n",
    "    def __init__(self, num_states, num_observations):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        @param num_unique_states: # of unique states\n",
    "        @param num_observations: # of unique observations\n",
    "        \"\"\"\n",
    "        self.num_states = num_states\n",
    "        self.num_observations = num_observations\n",
    "        self.transition_matrix = np.zeros((num_states, num_states))\n",
    "        self.initial_states_vector = np.zeros(num_states)\n",
    "        self.mu = None\n",
    "        self.sigma = None\n",
    "        \n",
    "    def initialize_em(self, A, pi, mu, sigma):\n",
    "        print(\"Initializing EM... \\n\")\n",
    "        \n",
    "        #  initialize state transition matrix (KxK matrix)\n",
    "        self.transition_matrix = A\n",
    "        print(\"Initial transition matrix: \\n\", self.transition_matrix)\n",
    "        \n",
    "        # initialize the initial prob vector (K-dim vector)\n",
    "        self.initial_states_vector = pi\n",
    "        print(\"Initial states vector: \", self.initial_states_vector)\n",
    "    \n",
    "        # initialize the emission probabilities (assuming Gaussian distributions)\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        print(\"Initial mu:\", self.mu)\n",
    "        print(\"Initial sigma\", self.sigma)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    def _one_normal_pdf(self, x, mu, sigma):\n",
    "        # prob = stats.norm.pdf(x, mu, sigma)\n",
    "        dx = 1e-4\n",
    "        cdf_2 = stats.norm.cdf(x+dx, mu, np.sqrt(sigma))\n",
    "        cdf_1 = stats.norm.cdf(x-dx, mu, np.sqrt(sigma))\n",
    "        return cdf_2 - cdf_1\n",
    "        \n",
    "    def _compute_alpha(self, X):\n",
    "        N = X.shape[0]\n",
    "        alpha = np.zeros((self.num_states, N)) # (K, N)\n",
    "        c = np.zeros(N)  # normalization constant\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            x_pdf_v = self._one_normal_pdf(X[0], self.mu[i], self.sigma[i])\n",
    "            alpha[i, 0] = self.initial_states_vector[i] * x_pdf_v\n",
    "        c[0] = np.sum(alpha[:, 0])\n",
    "        alpha[:, 0] /= c[0]\n",
    "\n",
    "        for t in range(1, N):\n",
    "            for i in range(self.num_states):\n",
    "                for j in range(self.num_states):\n",
    "                    alpha[i, t] += alpha[j, t-1] * self.transition_matrix[j, i]\n",
    "                alpha[i, t] *= self._one_normal_pdf(X[t], self.mu[i], self.sigma[i])\n",
    "            c[t] = np.sum(alpha[:, t])\n",
    "            alpha[:,t] /= c[t]\n",
    "        \n",
    "        return alpha, c\n",
    "    \n",
    "    def _compute_beta(self, X, c):\n",
    "        N = c.shape[0]\n",
    "        beta = np.zeros((self.num_states, N)) # (K, N)\n",
    "        for k in range(self.num_states):\n",
    "            beta[k, N-1] = 1.\n",
    "\n",
    "        for t in range(N-2, -1, -1):\n",
    "            for i in range(self.num_states):\n",
    "                for j in range(self.num_states):\n",
    "                    beta[i,t] += beta[j, t+1] * self.transition_matrix[i, j] * self._one_normal_pdf(X[t+1], self.mu[j], self.sigma[j])\n",
    "            beta[:,t] /= c[t+1]\n",
    "        return beta\n",
    "    \n",
    "    def fit(self, X, tot_it=2):\n",
    "        \"\"\"\n",
    "        Method that fits the model.\n",
    "        @param X: array-like with dimension [# of examples, # of length]\n",
    "        \"\"\"\n",
    "        \n",
    "        it = 0\n",
    "        prev_logl = -1e6\n",
    "        self.log_lh_it = []\n",
    "        while True:\n",
    "            it += 1\n",
    "            print(\"\\n\")\n",
    "            print(\"Starting iteration # \", it)\n",
    "            #####################################################################################################\n",
    "            \n",
    "            # EXPECTATION STEP\n",
    "            alpha, c = self._compute_alpha(X) # compute alpha\n",
    "            beta = self._compute_beta(X, c) # compute beta\n",
    "            gamma = alpha * beta\n",
    "            \n",
    "            N = X.shape[0]\n",
    "            Xi = np.zeros((self.num_states, self.num_states, N))\n",
    "            for t in range(1, N):\n",
    "                Xi[:,:,t] = (1/c[t])*alpha[:,t-1][None].T.dot(beta[:,t][None])*self.transition_matrix\n",
    "                for l in range(self.num_states):\n",
    "                    Xi[:,l,t] *= self._one_normal_pdf(X[t], self.mu[l], self.sigma[l])\n",
    "            \n",
    "            # MAXIMIZATION STEP\n",
    "            self.initial_states_vector = (gamma[:, 0] / np.sum(gamma[:, 0])).T  # update initial state\n",
    "            temp = np.sum(Xi[:, :, 1:], axis=2)\n",
    "            self.transition_matrix = temp / np.sum(temp, axis=1)[None].T # update transition matrix\n",
    "            \n",
    "            print(\"new initial conditions: \", self.initial_states_vector)\n",
    "            print(\"new transition matrix: \\n\", self.transition_matrix)\n",
    "            print(\"\\n\")\n",
    "\n",
    "            gamma_sum = np.sum(gamma, axis=1)[None].T\n",
    "            for k in range(self.num_states):  \n",
    "                self.mu[k] = np.sum(gamma[k, :] * X) / gamma_sum[k]\n",
    "                self.sigma[k] = np.sum(gamma[k, :] * X**2) / gamma_sum[k] - self.mu[k]**2\n",
    "            \n",
    "            print(\"new mu: \", self.mu)\n",
    "            print(\"new sigma: \", self.sigma)\n",
    "            \n",
    "            #####################################################################################################\n",
    "            \n",
    "            train_logl = np.sum(np.log(c))\n",
    "            self.log_lh_it.append(train_logl)\n",
    "            # need some stopping condition\n",
    "            if it >= tot_it:\n",
    "                break\n",
    "            print (f\"Completed iteration # {it}: log-likelihood diff = {np.abs(train_logl - prev_logl)}\")\n",
    "            prev_logl = train_logl\n",
    "        \n",
    "    \n",
    "    def decode_single_chain(self, X):\n",
    "        \"\"\"\n",
    "        Auxiliary method that uses Viterbi on single chain\n",
    "        @param X: array-like with dimension [ # of length]\n",
    "        @return z: array-like with dimension [# of length]\n",
    "        \"\"\"\n",
    "        # init holders\n",
    "        z = []\n",
    "        N = X.shape[0]\n",
    "        V = np.zeros((N, self.num_states))\n",
    "        best_states = np.zeros((N, self.num_states))\n",
    "        \n",
    "        #########################################\n",
    "        # TODO: implement the Viterbi algorithm #\n",
    "        #########################################\n",
    "        \n",
    "        for n in range(N):\n",
    "            if n == 0:\n",
    "                v_i_m1 = np.log(self.initial_states_vector)\n",
    "            else:\n",
    "                v_i_m1 = V[n-1, :]\n",
    "                \n",
    "            em_prob = np.zeros(self.num_states)\n",
    "            for k in range(self.num_states):\n",
    "                em_prob[k] = stats.norm.pdf(X[n], self.mu[k], np.sqrt(self.sigma[k]))\n",
    "                \n",
    "            v_i_all = v_i_m1[:, None] + np.log(self.transition_matrix) + np.log(em_prob)\n",
    "            best_idx = np.argmax(v_i_all, axis=0)\n",
    "                \n",
    "            V[n, :] = np.amax(v_i_all, axis=0)\n",
    "            best_states[n, :] = best_idx\n",
    "        \n",
    "        for n in range(N, 0, -1):\n",
    "            if n == N:\n",
    "                j = int(np.argmax(V[n-1, :]))\n",
    "                z.append(j)\n",
    "            else:\n",
    "                j = int(best_states[n, j])\n",
    "                z.append(j)\n",
    "\n",
    "        return list(reversed(z))\n",
    "        \n",
    "    def decode(self, X):\n",
    "        \"\"\"\n",
    "        Method that performs the Viterbi the model.\n",
    "        @param X: array-like with dimension [# of examples, # of length]\n",
    "        @return z: array-like with dimension [# of examples, # of length]\n",
    "        \"\"\"\n",
    "        return [self.decode_single_chain(sample) for sample in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My HMM: Latent Space dimension = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_states = 3\n",
    "num_obs = 1\n",
    "total_iter = 1000\n",
    "A = np.array([[.1, .7, .2],[.3, .2, .5],[.4, .4, .2]])\n",
    "PI = np.array([.2, .3, .5])\n",
    "mu = np.zeros(num_states)\n",
    "sigma = np.array([.1, .2, .3])\n",
    "my_hmm = MyHMM(num_states, num_obs)\n",
    "my_hmm.initialize_em(A, PI, mu, sigma)\n",
    "my_hmm.fit(data[:, 1], tot_it=total_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the log-likelihood during training\n",
    "plt.figure(figsize=(8, 6))\n",
    "log_l = my_hmm.log_lh_it\n",
    "plt.plot(range(1,len(log_l)+1), log_l, \"-b\")\n",
    "plt.xlabel(\"Iteration #\", fontsize=12)\n",
    "plt.ylabel(\"Log-likelihood\", fontsize=12)\n",
    "plt.grid()\n",
    "plt.title(\"EM with Latent space dimension = \"+str(my_hmm.num_states), fontsize=12)\n",
    "print(f\"Final Log-likelihood value after {total_iter} iterations and latent dim {my_hmm.num_states} is: {log_l[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Viterbi to decode the data, k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_labels(hmm_model, data_):\n",
    "    # plot labels of latent variables we found by viterbi\n",
    "    data = data_[:, 1]\n",
    "    N = data.shape[0]\n",
    "    z_seq = np.array(hmm_model.decode_single_chain(data))+1\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(np.arange(N), data, 'k-.', linewidth = 0.5)\n",
    "    plt.grid()\n",
    "    colors = ['k', 'r', 'b']\n",
    "    plt.title(\"Latent space dimension = \"+str(my_hmm.num_states), fontsize=12)\n",
    "    for i in range(N):\n",
    "        plt.text(i, data[i], z_seq[i], c=colors[z_seq[i]-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_labels(my_hmm, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final results with k = {num_states}: \\n\")\n",
    "print(\"Transition matrix: \\n\", my_hmm.transition_matrix)\n",
    "print(\"Initial state vector: \", my_hmm.initial_states_vector)\n",
    "print(\"MU: \", my_hmm.mu)\n",
    "print(\"Sigma: \", np.sqrt(my_hmm.sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the picture above and the final parameters after 1000 iterations of EM for the case k=3, the results are very similar to the ones reported in the book. In addition, we can see that our EM implementation converges to very similar values for the HMM's parameters (A,PI,mu,sigma). Finally, the maximum log-likelihood value is -3098."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My HMM: Latent Space dimension = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_states = 2\n",
    "num_obs = 1\n",
    "total_iter = 1000\n",
    "A = np.array([[.4, .6], [.2, .8]])\n",
    "PI = np.array([.4, .6])\n",
    "mu = np.zeros(num_states)\n",
    "sigma = np.array([.1, .2])\n",
    "my_hmm_2 = MyHMM(num_states, num_obs)\n",
    "my_hmm_2.initialize_em(A, PI, mu, sigma)\n",
    "my_hmm_2.fit(data[:, 1], tot_it=total_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the log-likelihood during training\n",
    "plt.figure(figsize=(8, 6))\n",
    "log_l = my_hmm_2.log_lh_it\n",
    "plt.plot(range(1,len(log_l)+1), log_l, \"-b\")\n",
    "plt.xlabel(\"Iteration #\", fontsize=12)\n",
    "plt.ylabel(\"Log-likelihood\", fontsize=12)\n",
    "plt.grid()\n",
    "plt.title(\"EM with Latent space dimension = \"+str(my_hmm_2.num_states), fontsize=12)\n",
    "print(f\"Final Log-likelihood value after {total_iter} iterations and latent dim {my_hmm_2.num_states} is: {log_l[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Viterbi to decode the data, k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_labels(my_hmm_2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final results with k = {num_states}: \\n\")\n",
    "print(\"Transition matrix: \\n\", my_hmm_2.transition_matrix)\n",
    "print(\"Initial state vector: \", my_hmm_2.initial_states_vector)\n",
    "print(\"MU: \", my_hmm_2.mu)\n",
    "print(\"Sigma: \", np.sqrt(my_hmm_2.sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From results with latent space dimension k=2 after running 1000 iterations of EM, we reach the maximum log-likelihood values of -3106. In the case of k=3, the maximum log-likelihood values is -3098, which is greater than the value for k=2. Hence, we can conlcude that the HMM with latent dimension k=3 performs better than the case k=2 (although the the gap among the two models in terms of log-likelihood is not that high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
